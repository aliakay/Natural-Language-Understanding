{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "corpus.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAsXZ6UqTgVC"
      },
      "source": [
        "# Corpus and Lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO5tRm5XTgW3"
      },
      "source": [
        "- __author__: Evgeny A. Stepanov\n",
        "- __e-mail__: stepanov.evgeny.a@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufvPgen6TgW4"
      },
      "source": [
        "Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/)) is advised for reading. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL9e9oG3TgW4"
      },
      "source": [
        "- Section *Corpora and Counting* covers some concepts of *Chapter 2: \"Regular Expressions, Text Normalization, Edit Distance\"*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZHHLd_mTgW5"
      },
      "source": [
        "__Requirements__\n",
        "\n",
        "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset\n",
        "\n",
        "    - run `git clone https://github.com/esrel/NL2SparQL4NLU.git`\n",
        "    \n",
        "- [spaCy](https://spacy.io/)\n",
        "    - run `pip install spacy`\n",
        "    - run `python -m spacy download en` to install English language model\n",
        "    \n",
        "- [NLTK](http://www.nltk.org/)\n",
        "    - run `pip install nltk`\n",
        "\n",
        "- [scikit-learn](https://scikit-learn.org/)\n",
        "    - run `pip install scikit-learn`\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKN0C-7JT-rI",
        "outputId": "e75ba5a8-4b62-4da5-eabd-186a87b8cb27"
      },
      "source": [
        "!git clone https://github.com/esrel/NL2SparQL4NLU.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NL2SparQL4NLU'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 80 (delta 2), reused 17 (delta 0), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (80/80), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVJO6SjDTgW8"
      },
      "source": [
        "__Alternative Corpora__\n",
        "\n",
        "Use __only__ if you know how to work with JSON!\n",
        "\n",
        "- https://github.com/howl-anderson/ATIS_dataset\n",
        "- https://github.com/sebischair/NLU-Evaluation-Corpora\n",
        "- https://github.com/sonos/nlu-benchmark\n",
        "- https://github.com/clinc/oos-eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRHAU19STgXE"
      },
      "source": [
        "## 1. Corpora and Counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwmQhiBJTgXF"
      },
      "source": [
        "### 1.1. Corpus\n",
        "\n",
        "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
        "\n",
        "__Corpus Properties__:\n",
        "- *Format* -- how to read/load it?\n",
        "- *Language* -- which tools/models can I use?\n",
        "- *Annotation* -- what it is intended for?\n",
        "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
        "\n",
        "| Set         | Purpose                                       |\n",
        "|:------------|:----------------------------------------------|\n",
        "| Training    | training model, extracting rules, etc.        |\n",
        "| Development | tuning, optimization, intermediate evaluation |\n",
        "| Test        | final evaluation (remains unseen)             |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzlUuYI-TgXF"
      },
      "source": [
        "#### NL2SparQL4NLU\n",
        "\n",
        "- __Format__:\n",
        "\n",
        "    - Utterance (sentence) per line\n",
        "    - Tokenized\n",
        "    - Lowercased\n",
        "\n",
        "- __Language__: English monolingual\n",
        "\n",
        "- __Annotation__: None (for now)\n",
        "\n",
        "- __Split__: training & test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoOVAY6YTgXG"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "- define a function to load a corpus into a list-of-lists\n",
        "\n",
        "- load `NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt`\n",
        "- print first `2` tokens of the first `10` utterances\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYpkCzC4TgXJ"
      },
      "source": [
        "trn='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
        "tst='NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7L83IlmUEL2",
        "outputId": "3d72c35b-b37c-4f32-f5b9-e923b6341958"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NL2SparQL4NLU  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "TQEkrG5bTgXY",
        "outputId": "ca069bab-6967-46a7-9432-bd41bcc3af79"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(trn, delimiter = \"\\t\")\n",
        "df = df.T.reset_index(drop=True).T\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>show credits for the godfather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>who was the main actor in the exorcist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>find the female actress from the movie she 's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>who played dory on finding nemo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who was the female lead in resident evil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3332</th>\n",
              "      <td>how many oscars has meryl streep won</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3333</th>\n",
              "      <td>how many of the star wars movies won awards</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3334</th>\n",
              "      <td>play english movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3335</th>\n",
              "      <td>were there any movies that came out this year ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3336</th>\n",
              "      <td>what are some popular japanese movies</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3337 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0\n",
              "0                        show credits for the godfather\n",
              "1                who was the main actor in the exorcist\n",
              "2     find the female actress from the movie she 's ...\n",
              "3                       who played dory on finding nemo\n",
              "4              who was the female lead in resident evil\n",
              "...                                                 ...\n",
              "3332               how many oscars has meryl streep won\n",
              "3333        how many of the star wars movies won awards\n",
              "3334                                 play english movie\n",
              "3335  were there any movies that came out this year ...\n",
              "3336              what are some popular japanese movies\n",
              "\n",
              "[3337 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "nS7nI2wXVnze",
        "outputId": "d6defd4b-c061-4d1a-82ee-d3202c098658"
      },
      "source": [
        "df_test = pd.read_csv(tst, delimiter = \"\\t\")\n",
        "df_test = df_test.T.reset_index(drop=True).T\n",
        "df_test"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>who is in the movie the campaign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>list the cast of the movie the campaign</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>who was in twilight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>who is in vulguria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>actor from lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>trailer for star wars a new hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1079</th>\n",
              "      <td>show resident evil movies with trailers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>can i see previews for upcoming warner brother...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1081</th>\n",
              "      <td>how many woody allen movies are set in new yor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082</th>\n",
              "      <td>how many scorsese films were filmed in france</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1083 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0\n",
              "0                      who is in the movie the campaign\n",
              "1               list the cast of the movie the campaign\n",
              "2                                   who was in twilight\n",
              "3                                    who is in vulguria\n",
              "4                                       actor from lost\n",
              "...                                                 ...\n",
              "1078                   trailer for star wars a new hope\n",
              "1079            show resident evil movies with trailers\n",
              "1080  can i see previews for upcoming warner brother...\n",
              "1081  how many woody allen movies are set in new yor...\n",
              "1082      how many scorsese films were filmed in france\n",
              "\n",
              "[1083 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8FTqUhcULVd",
        "outputId": "6fb25f6f-dafd-4fbe-dd2d-e143923fcc6f"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2f4mOqhUPap",
        "outputId": "c3114043-2544-4922-ce4c-db99c42dbd3b"
      },
      "source": [
        "for l in range(10):\n",
        "  tokenized_sents = [word_tokenize(df.values[l][0])[:2]]\n",
        "  for i in tokenized_sents:\n",
        "      print(i)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['show', 'credits']\n",
            "['who', 'was']\n",
            "['find', 'the']\n",
            "['who', 'played']\n",
            "['who', 'was']\n",
            "['who', 'played']\n",
            "['who', 'was']\n",
            "['find', 'the']\n",
            "['cast', 'and']\n",
            "['cast', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMjZtlJZUPlJ",
        "outputId": "c84697a8-3fbd-433f-9cb3-c4774a8d6ab8"
      },
      "source": [
        "tokenized_sents[:1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['cast', 'and']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByCqbre-UPvC"
      },
      "source": [
        "rows=word_tokenize(df.values[1][0])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exq_tMSRZU9I",
        "outputId": "66839974-df9a-48af-a276-18baf8f92d31"
      },
      "source": [
        "len(word_tokenize(df.values[1][0]))+1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsNlILIXTgXm"
      },
      "source": [
        "### 1.2. Corpus Descriptive Statistics (Counting)\n",
        "\n",
        "*Corpus* description in terms of:\n",
        "\n",
        "- total number of tokens\n",
        "- total number of utterances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJFohukKTgXn"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "- define a function to compute corpus descriptive statistics -- number of utterance and token counts\n",
        "- compute the statistics for the __training__ and __test__ sets of NL2SparQL4NLU dataset. \n",
        "- compare the computed statistics with the reference values below.\n",
        "\n",
        "\n",
        "| Metric           | Train  | Test   |\n",
        "|------------------|-------:|-------:|\n",
        "| Total Tokens     | 21,453 |  7,117 |\n",
        "| Total Utterances |  3,338 |  1,084 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "f9tVFi9XaWCm",
        "outputId": "0dda1892-2eb2-4f7d-cb13-609293731603"
      },
      "source": [
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>show credits for the godfather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>who was the main actor in the exorcist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>find the female actress from the movie she 's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>who played dory on finding nemo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who was the female lead in resident evil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3332</th>\n",
              "      <td>how many oscars has meryl streep won</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3333</th>\n",
              "      <td>how many of the star wars movies won awards</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3334</th>\n",
              "      <td>play english movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3335</th>\n",
              "      <td>were there any movies that came out this year ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3336</th>\n",
              "      <td>what are some popular japanese movies</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3337 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0\n",
              "0                        show credits for the godfather\n",
              "1                who was the main actor in the exorcist\n",
              "2     find the female actress from the movie she 's ...\n",
              "3                       who played dory on finding nemo\n",
              "4              who was the female lead in resident evil\n",
              "...                                                 ...\n",
              "3332               how many oscars has meryl streep won\n",
              "3333        how many of the star wars movies won awards\n",
              "3334                                 play english movie\n",
              "3335  were there any movies that came out this year ...\n",
              "3336              what are some popular japanese movies\n",
              "\n",
              "[3337 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gORaaLXab8d",
        "outputId": "0ede9440-ce82-4b14-df4c-58a95e1af6d9"
      },
      "source": [
        "word_tokenize(df.values[0][0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['show', 'credits', 'for', 'the', 'godfather']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30q96KPtcCD0"
      },
      "source": [
        "def descriptive(dataset):\n",
        "  #Dataset should be dataframe\n",
        "  a=0\n",
        "  for l in range(0,len(dataset)):\n",
        "    tokenized_sents = [word_tokenize(dataset.values[l][0])]\n",
        "    a=a+(len(word_tokenize(dataset.values[l][0])))\n",
        "  print(\"Number of Token:\",a)\n",
        "  print(\"Number of utterance:\",len(df))\n",
        "  print(\"Average token per utterance\",float(a/len(dataset)))\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNSNb-OicbMX",
        "outputId": "4e90ea1e-0daa-4b38-a1db-64686f0c7782"
      },
      "source": [
        "descriptive(df)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Token: 21448\n",
            "Number of utterance: 3337\n",
            "Average token per utterance 6.427329937069224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1c0Sctkcq2Q",
        "outputId": "fab30d95-e256-43ce-c91b-9c1358c244ef"
      },
      "source": [
        "descriptive(df_test)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Token: 7117\n",
            "Number of utterance: 3337\n",
            "Average token per utterance 6.571560480147737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKjhAWc2TgXs"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "- define a function to compute average token per utterance statistic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkzaMEmETgXw"
      },
      "source": [
        "## 2. Lexicon\n",
        "\n",
        "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
        "\n",
        "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
        "\n",
        "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHj4OohTgXx"
      },
      "source": [
        "### 2.1. Lexicon Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdcOAWkBTgX1"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "- define a function to compute a lexicon from corpus in a list-of-lists format\n",
        "    - sort the list alphabetically\n",
        "    \n",
        "- compute the lexicon of the training set of NL2SparQL4NLU dataset\n",
        "- compare its size to the reference value below.\n",
        "\n",
        "| Metric       | Value |\n",
        "|--------------|------:|\n",
        "| Lexicon Size | 1,729 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJKBy_VbTgX2",
        "outputId": "2cbf5747-542a-4b0a-f4fe-932d31088a96"
      },
      "source": [
        "count = {}\n",
        "for l in range(0,len(df)):\n",
        "  tokenized_sents = [word_tokenize(df.values[l][0])]\n",
        "  for word in tokenized_sents[0]:\n",
        "    if word in count:\n",
        "      count[word] += 1\n",
        "    else:\n",
        "      count[word] = 1\n",
        "print(\"Lexicon Size:\",len(count))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lexicon Size: 1729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lDIgXo2TgX2"
      },
      "source": [
        "### 2.2. Frequency List\n",
        "\n",
        "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
        "\n",
        "What is a \"word\"?\n",
        "\n",
        "- case sensitive counts\n",
        "- case insensitive counts (our corpus is lowercased)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Kr6V0BTgX3"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "- define a function to compute a frequency list for a corpus\n",
        "- compute frequency list for the training set of NL2SparQL4NLU dataset\n",
        "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
        "- compare the frequencies to the reference values below\n",
        "\n",
        "| Word   | Frequency |\n",
        "|--------|----------:|\n",
        "| the    |     1,337 |\n",
        "| movies |     1,126 |\n",
        "| of     |       607 |\n",
        "| in     |       582 |\n",
        "| movie  |       564 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM0i092RTgX6"
      },
      "source": [
        "def nbest(d, n=1):\n",
        "    \"\"\"\n",
        "    get n max values from a dict\n",
        "    :param d: input dict (values are numbers, keys are stings)\n",
        "    :param n: number of values to get (int)\n",
        "    :return: dict of top n key-value pairs\n",
        "    \"\"\"\n",
        "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeapAu85phlF"
      },
      "source": [
        "def freaquency(df):\n",
        "  fre=df[0].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
        "  fre=pd.DataFrame(fre)\n",
        "  fre.columns=[\"Frequency\"]\n",
        "  print(fre.sort_values(\"Frequency\",ascending=False).head(10))\n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MvlUNDOpuKq",
        "outputId": "91e19540-1e60-4512-8301-c63e7fb2b77e"
      },
      "source": [
        "freaquency(df)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Frequency\n",
            "the        1337.0\n",
            "movies     1126.0\n",
            "of          607.0\n",
            "in          582.0\n",
            "movie       564.0\n",
            "what        545.0\n",
            "me          539.0\n",
            "show        494.0\n",
            "for         472.0\n",
            "is          335.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3xvpNycptsU",
        "outputId": "fe5d5d1e-9f67-43fb-cc1f-041615c5ed85"
      },
      "source": [
        "freaquency(df_test)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Frequency\n",
            "the         406.0\n",
            "movies      367.0\n",
            "movie       200.0\n",
            "of          193.0\n",
            "in          184.0\n",
            "what        178.0\n",
            "me          160.0\n",
            "for         146.0\n",
            "show        141.0\n",
            "find        117.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EHYT85GTgX7"
      },
      "source": [
        "### 2.3. Lexicon Operations\n",
        "\n",
        "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*).\n",
        "\n",
        "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
        "\n",
        "Any group of words can be chosen as the stop words for a given purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkVJbUD5TgX8"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG7Ct4RoTgX-"
      },
      "source": [
        "##### Frequency Cut-Off\n",
        "\n",
        "- define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
        "\n",
        "    - use default values for min and max\n",
        "    \n",
        "- using frequency list for the training set of NL2SparQL4NLU dataset\n",
        "    \n",
        "    - compute lexicon applying:\n",
        "    \n",
        "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
        "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
        "        - both minimum and maximum thresholds together\n",
        "        \n",
        "    - report size for each comparing to the reference values in the table\n",
        "\n",
        "| Operation  | Min | Max | Size |\n",
        "|------------|----:|----:|-----:|\n",
        "| original   | N/A | N/A | 1729 |\n",
        "| cut-off    |   2 | N/A |  950 |\n",
        "| cut-off    | N/A | 100 | 1694 |\n",
        "| cut-off    |   2 | 100 |  915 |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlluL1duTgX_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bD7Lm01TgYB"
      },
      "source": [
        "##### Stop Word Removal\n",
        "\n",
        "- define a function to read/load a list of words in token-per-line format (i.e. lexicon)\n",
        "- load stop word list from `NL2SparQL4NLU/extras/english.stop.txt`\n",
        "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
        "    \n",
        "    - define a function to compute overlap of two lexicons\n",
        "    - define a function to apply a stopword list to a lexicon\n",
        "\n",
        "- compare the 100 most frequent words in frequency list of the training set to the list of stopwords (report count)\n",
        "- apply stopword list to the lexicon of the training set\n",
        "- report size of the resulting lexicon comparing to the reference values.\n",
        "\n",
        "| Operation       | Size |\n",
        "|-----------------|-----:|\n",
        "| original        | 1729 |\n",
        "| no stop words   | 1529 |\n",
        "| top 100 overlap |   50 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ie0-ZVbTgYE"
      },
      "source": [
        "swl='NL2SparQL4NLU/extras/english.stop.txt'"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WDOHXgPBTgYF",
        "outputId": "70d237bd-225f-4642-b36a-17c77e4dfda9"
      },
      "source": [
        "df_stop = pd.read_csv(swl, delimiter = \"\\t\")\n",
        "df_stop = df_stop.T.reset_index(drop=True).T\n",
        "df_stop.head()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a's</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>able</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>about</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>above</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>according</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0\n",
              "0        a's\n",
              "1       able\n",
              "2      about\n",
              "3      above\n",
              "4  according"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ZeV3UxvEOY"
      },
      "source": [
        "stopwords=df_stop[0].values.tolist()"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4DvQkstvtsQ"
      },
      "source": [
        "train=df[0].values.tolist()"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "4nr0MtvMv1dD",
        "outputId": "39555f34-ac13-4825-ba25-9a6815c3e7eb"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'show credits for the godfather'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roagUFYkvshC"
      },
      "source": [
        "text_tokens = word_tokenize(train[0])\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords]"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3LFidz_tdR3",
        "outputId": "8bbebc40-fb7a-41e9-d614-a2beb0bcdfc9"
      },
      "source": [
        "count = {}\n",
        "for l in range(0,len(df)):\n",
        "  text_tokens = word_tokenize(train[l])\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
        "  for word in tokens_without_sw:\n",
        "    if word in count:\n",
        "      count[word] += 1\n",
        "    else:\n",
        "      count[word] = 1\n",
        "print(\"No Stop Word:\",len(count))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Stop Word: 1531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5K_XkmPtdml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XXtEfTiTgYF"
      },
      "source": [
        "##### Exercise: Alternative Stop Words\n",
        "\n",
        "Compare the stop word list above to the stop word lists from the popular python libraries in terms of overlaps.\n",
        "(Use `set` `intersection`)\n",
        "\n",
        "- spaCy\n",
        "- NLTK\n",
        "- scikit-learn\n",
        "\n",
        "    \n",
        "For NLTK you need to download them first\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyoyO3l_TgYG",
        "outputId": "1f9cf801-bd83-459e-f87c-4df2132bd469"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
        "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
        "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))\n"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "spaCy: 326\n",
            "NLTK: 179\n",
            "sklearn: 318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUmWRTAATgYJ",
        "outputId": "17269079-56dd-4941-f61f-0374c37f2592"
      },
      "source": [
        "count = {}\n",
        "for l in range(0,len(df)):\n",
        "  text_tokens = word_tokenize(train[l])\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  for word in tokens_without_sw:\n",
        "    if word in count:\n",
        "      count[word] += 1\n",
        "    else:\n",
        "      count[word] = 1\n",
        "print(\"No Stop Word:\",len(count))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Stop Word: 1602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm4SAB1gTgYN"
      },
      "source": [
        "## 3. Basic Text Pre-processing\n",
        "\n",
        "Both frequency cut-off and stop word removal are frequently used text pre-processing steps. Depending on the application, there are several other common text pre-processing steps that are usually applied for tranforming text for Machine Learning tasks.\n",
        "\n",
        "__Text Normalization Steps__\n",
        "\n",
        "- removing extra white spaces\n",
        "\n",
        "- tokenization\n",
        "    - documents to sentences (sentence segmentation/tokenization)\n",
        "    - sentences to tokens\n",
        "\n",
        "- lowercasing/uppercasing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p61-CUPjTgYP"
      },
      "source": [
        "- removing punctuation\n",
        "\n",
        "- removing accent marks and other diacritics \n",
        "\n",
        "- removing stop words (see above)\n",
        "\n",
        "- removing sparse terms (frequency cut-off)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWotbxmmTgYQ"
      },
      "source": [
        "- number normalization\n",
        "    - numbers to words (i.e. `10` to `ten`)\n",
        "    - number words to numbers (i.e. `ten` to `10`)\n",
        "    - removing numbers\n",
        "\n",
        "- verbalization (specifically for speech applications)\n",
        "\n",
        "    - numbers to words\n",
        "    - expanding abbreviations (or spelling out)\n",
        "    - reading out dates, etc.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKZMbtCSTgYU"
      },
      "source": [
        "- [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\n",
        "    - the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
        "\n",
        "- [stemming](https://en.wikipedia.org/wiki/Stemming)\n",
        "    - the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5xe3zuvTgYY"
      },
      "source": [
        "### 3.1. Tokenization and Lemmatization with spaCy\n",
        "\n",
        "The default spaCy NLP pipeline does several processing steps including __tokenization__, *part of speech tagging*, __lemmatization__, *dependency parsing* and *Named Entity Recognition* (ignore the ones in *italics* for today). \n",
        "\n",
        "\n",
        "SpaCy produces a `Doc` object that contains `Token`s. It is possible to access lemmatized form of a token using its `lemma_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dSxg_yNTgYZ",
        "outputId": "7d87ba85-7f4a-4b07-f4cb-8e3bcd2807b2"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "txt = 'who plays luke on star wars new hope'\n",
        "doc = nlp(txt)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "print(lemmas)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who', 'play', 'luke', 'on', 'star', 'war', 'new', 'hope']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhU6GmjXzC5u",
        "outputId": "14c52da1-bb30-4724-bb4c-9bd573374dd7"
      },
      "source": [
        "doc"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "who plays luke on star wars new hope"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaH6hE9HTgYc"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "- Lemmatize the dataset with spaCy\n",
        "- compute the lexicon of the training set of NL2SparQL4NLU dataset (or the one you have chosen)\n",
        "- compare its size to the \"raw\" counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "fWPzgLEAzZeZ",
        "outputId": "19db56d8-a699-4af1-872d-99eca7f19651"
      },
      "source": [
        "train[1]"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'who was the main actor in the exorcist'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjYzPEyTTgYc"
      },
      "source": [
        "lem=[]\n",
        "for l in range(0,len(df)):\n",
        "  txt=train[l]\n",
        "  doc=nlp(txt)\n",
        "  lemmas = [token.lemma_ for token in doc]\n",
        "  lem=lem+lemmas"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxlt2_DJ0KEB",
        "outputId": "0806f967-b758-4c47-e638-9f0f98c14793"
      },
      "source": [
        "lem[30:50]"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who',\n",
              " 'be',\n",
              " 'the',\n",
              " 'female',\n",
              " 'lead',\n",
              " 'in',\n",
              " 'resident',\n",
              " 'evil',\n",
              " 'who',\n",
              " 'play',\n",
              " 'guido',\n",
              " 'in',\n",
              " 'life',\n",
              " 'be',\n",
              " 'beautiful',\n",
              " 'who',\n",
              " 'be',\n",
              " 'the',\n",
              " 'co',\n",
              " '-']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rpZEB6nTgYd"
      },
      "source": [
        "### 3.2. Stemming with NLTK\n",
        "\n",
        "SpaCy does not provide any stemming algorithms.\n",
        "NLTK, on the other hand, provides two algorithms [`Porter Stemmer`](https://tartarus.org/martin/PorterStemmer/) and [`Snowball Stemmer`](https://snowballstem.org/algorithms/) (a.k.a. `Porter2`). \n",
        "\n",
        "__Note__: Please read the original description of the algorithmsm, if you are interested.\n",
        "\n",
        "Since stemming works on token level, we need to provide tokens. Which we can obtain either from `spacy`'s `Doc` or just *whitespace tokenization*\n",
        "\n",
        "```python\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```python\n",
        "tokens = text.split()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwUI9GRBTgal",
        "outputId": "69f0967c-1d4e-4a12-c310-c617127b44b0"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "txt = 'who plays luke on star wars new hope'\n",
        "tokens = txt.split()\n",
        "print(tokens)\n",
        "\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who', 'plays', 'luke', 'on', 'star', 'wars', 'new', 'hope']\n",
            "['who', 'play', 'luke', 'on', 'star', 'war', 'new', 'hope']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-VBJNf9Tgao"
      },
      "source": [
        "#### Exercise\n",
        "- Stem the dataset with NTLK\n",
        "- compute the lexicon of the training set of NL2SparQL4NLU dataset (or the one you have chosen)\n",
        "- compare its size to the \"raw\" and lemmatized counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZBWMyjeTgap"
      },
      "source": [
        "st=[]\n",
        "for l in range(0,len(df)):\n",
        "  txt=train[l]\n",
        "  tokens = txt.split()\n",
        "  stems = [stemmer.stem(token) for token in tokens]\n",
        "  st=st+stems"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUF6w_EyTgaq",
        "outputId": "9d03a0eb-c12d-4d23-ba6a-380ac42d572b"
      },
      "source": [
        "st[:20]"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['show',\n",
              " 'credit',\n",
              " 'for',\n",
              " 'the',\n",
              " 'godfath',\n",
              " 'who',\n",
              " 'wa',\n",
              " 'the',\n",
              " 'main',\n",
              " 'actor',\n",
              " 'in',\n",
              " 'the',\n",
              " 'exorcist',\n",
              " 'find',\n",
              " 'the',\n",
              " 'femal',\n",
              " 'actress',\n",
              " 'from',\n",
              " 'the',\n",
              " 'movi']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WorxFgtH0hmi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}