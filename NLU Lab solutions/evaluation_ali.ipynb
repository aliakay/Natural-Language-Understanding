{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "evaluation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz-IpPs0awqK"
      },
      "source": [
        "# Evaluation in Natural Language Processing\n",
        "\n",
        "- Evgeny A. Stepanov\n",
        "- stepanov.evgeny.a@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbPKqwreawqV"
      },
      "source": [
        "__Requirements__\n",
        "- [scikit-learn](https://scikit-learn.org/)\n",
        "    - run `pip install scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FyLkf7YawqW"
      },
      "source": [
        "## Take-Aways\n",
        "- Basics of evaluation in NLP\n",
        "- Running classification experiment with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSmHV0nyawqW"
      },
      "source": [
        "## 1. Basic Concepts\n",
        "\n",
        "__Why do we want to evaluate a system / an algorithm's performance?__\n",
        "\n",
        "- To measure one or more of its qualities.\n",
        "- Proper evaluation criteria is a way to specify the problem.\n",
        "\n",
        "__How do we evaluate a system / an algorithm's performance?__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iFBaUQmawqW"
      },
      "source": [
        "### 1.1. Automatic vs. Manual Evaluation\n",
        "\n",
        "#### Automatic Evaluation\n",
        "Compare the system’s output with the gold standard (reference)\n",
        "- _Cons_: An effort to produce the gold standard (manual)\n",
        "- _Pros_: Re-usable; no additional cost\n",
        "- __OBJECTIVE__\n",
        "\n",
        "#### Manual Evaluation\n",
        "Ask human judges to estimate the quality w.r.t. certain criteria\n",
        "- For some tasks the gold standard might be unobtainable\n",
        "- No agreed automatic evaluation method\n",
        "- __SUBJECTIVE__\n",
        "              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1i5XurGawqX"
      },
      "source": [
        "### 1.2. Intrinsic vs. Extrinsic Evaluation\n",
        "\n",
        "#### Intrinsic\n",
        "- in isolation\n",
        "- w.r.t. gold standard (references)\n",
        "e.g. POS-Tagging performance\n",
        "\n",
        "#### Extrinsic\n",
        "- as a part of other system\n",
        "- usefulness for some other task\n",
        "e.g. effect of POS-Tagger on parsing performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhrOT0NvawqX"
      },
      "source": [
        "### 1.3. Black-Box vs. Glass-Box\n",
        "\n",
        "#### Black-Box\n",
        "Evaluation of Performance:\n",
        "- speed\n",
        "- accuracy \n",
        "- etc.\n",
        "\n",
        "#### Glass-Box\n",
        "Evaluation of Design: \n",
        "- algorithm\n",
        "- used resources \n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyeODrSxawqY"
      },
      "source": [
        "### 1.4. Gold Standard / References\n",
        "__Where Gold Standard comes from?__\n",
        "- Annotation by experts (human judges)\n",
        "\n",
        "__How do we know that Gold Standard is good?__\n",
        "- Evaluate agreement between the annotators/judges\n",
        "- Most simple agreement measure: % of agreed instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlISJRvawqY"
      },
      "source": [
        "### 1.5. Lower & Upper Bounds of the Performance\n",
        "\n",
        "#### Lower Bound: Baseline\n",
        "Trivial solution to the problem: \n",
        "\n",
        "- _random_: random decision\n",
        "- _chance_: random decision w.r.t. the distribution of categories in the training data\n",
        "- _majority_: assign everything to the largest category etc.\n",
        "\n",
        "#### Upper Bound: Inter-rater agreement\n",
        "Usually human performance.\n",
        "\n",
        "A system is expected to perform within the lower and upper bounds.\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPmQxCL7awqZ"
      },
      "source": [
        "### 1.6. Data Split\n",
        "\n",
        "- _Training_: for training / extracting rules / etc.\n",
        "- _Development_ (_Validation_): for optimization / intermediate evaluation\n",
        "- _Testing_: for the final evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_31MpuTawqZ"
      },
      "source": [
        "#### 1.6.1. [K-Fold Cross-Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n",
        "In k-fold cross-validation, the original sample is randomly partitioned into $k$ equal sized subsamples. Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining $k − 1$ subsamples are used as training data. The cross-validation process is then repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data. The $k$ results can then be averaged to produce a single estimation.\n",
        "\n",
        "- Random K-Fold Cross-Validation splits data into $K$ equal folds\n",
        "- Stratified K-Fold Cross-Validation additionally makes sure that the distribution of target labels is similar across different folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo7EJCBHawqZ"
      },
      "source": [
        "The general procedure is as follows:\n",
        "\n",
        "- Shuffle the dataset randomly\n",
        "- Split the dataset into $k$ folds\n",
        "- For each unique group:\n",
        "    - Take the group as a hold out or test data set\n",
        "    - Take the remaining groups as a training data set\n",
        "    - Fit a model on the training set and evaluate it on the test set\n",
        "    - Retain the evaluation score and discard the model\n",
        "- Summarize the model performance averaging the evaluation scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdnDeDdawqa"
      },
      "source": [
        "## 2. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhb7bIGpawqa"
      },
      "source": [
        "### 2.1. Contingency Table\n",
        "\n",
        "A [contingency table](https://en.wikipedia.org/wiki/Contingency_table) (also known as a _cross tabulation_ or _crosstab_) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables. For the binary classification into positive (_POS_) and negative (_NEG_) classes, the predictions of a model (_HYP_, for hypotheses) with respect to the true labels (_REF_, for referencens) can be represented as the  matrix.\n",
        "\n",
        "|     |         | REF     |         |\n",
        "|-----|---------|:-------:|:-------:|\n",
        "|     |         | __POS__ | __NEG__ |\n",
        "| HYP | __POS__ | TP      | FP      |\n",
        "|     | __NEG__ | FN      | TN      |\n",
        "\n",
        "\n",
        "Where:\n",
        "- __TP__: True Positives (usually denoted as $a$)\n",
        "- __FP__: False Positivea ($b$)\n",
        "- __FN__: False Negatives ($c$)\n",
        "- __TN__: True Negativea ($d$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd3HPWqKawqb"
      },
      "source": [
        "### 2.1. The Simplest Case: Accuracy\n",
        "\n",
        "$$ \\text{Accuracy} = \\frac{\\text{Num. of Correct Decisions}}{\\text{Total Num. of Instances}} $$\n",
        "\n",
        "- Known number of instances\n",
        "- Single decision for each instance \n",
        "- Single correct answer for each instance \n",
        "- All errors are equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caQvhoKhawqc"
      },
      "source": [
        "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nkrT6Nmawqd"
      },
      "source": [
        "__What if TN is infinite or unknown?__\n",
        "\n",
        "e.g.: Number of irrelevant queries to a search engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mofs5OUVawqe"
      },
      "source": [
        "### 2.2. Precision & Recall\n",
        "\n",
        "|     |         | REF     |         |             |\n",
        "|-----|---------|:-------:|:-------:|-------------|\n",
        "|     |         | __POS__ | __NEG__ |             |\n",
        "| HYP | __POS__ | TP      | FP      | _Precision_ |\n",
        "|     | __NEG__ | FN      | TN      |             |\n",
        "|     |         | _Recall_ |        |             |\n",
        "\n",
        "\n",
        "$$ \\text{Precison} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
        "\n",
        "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
        "\n",
        "__2 Values__: \n",
        "\n",
        "Precision-Recall Trade-Off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZC3pyl9awqe"
      },
      "source": [
        "### 2.3. F-Measure\n",
        "\n",
        "- Harmonic Mean of Precision & Recall \n",
        "- Usually evenly weighted\n",
        "\n",
        "\n",
        "$$ F_{\\beta} = \\frac{(1 + \\beta^2) ∗ \\text{Precision} ∗ \\text{Recall}}{\\beta^2 ∗ \\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "Most common value of $\\beta = 1$\n",
        "\n",
        "$$ F_1 = \\frac{2 ∗ \\text{Precision} ∗ \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYSK1JAEawqf"
      },
      "source": [
        "### 2.4. Micro, Macro and (Macro-) Weighted Averaging\n",
        "\n",
        "In a Multi-Class setting per-class scores are averaged to produce a single score.\n",
        "There are several ways the scores could be averaged. \n",
        "\n",
        "__Micro Averaging__\n",
        "\n",
        "We compute scores summing over True Positive, True Negative, False Positive and False Negatives.\n",
        "\n",
        "__Macro Averaging__\n",
        "\n",
        "We first compute scores per class, then average the scores ignoring their distribution in the test set.\n",
        "\n",
        "__(Macro-) Weighted Averaging__\n",
        "\n",
        "Similar to Macro Averaging, but we additionally weight the scores by the class-frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A95mTkBdawqg"
      },
      "source": [
        "#### Precision Example\n",
        "\n",
        "Let's assume we have 3 classes. The precision formula from above is:\n",
        "\n",
        "$$ \\text{Precison} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
        "\n",
        "$$\\text{Micro Precision} = \\frac{\\text{TP}_1 + \\text{TP}_2 +\\text{TP}_3}{(\\text{TP}_1 + \\text{TP}_2 +\\text{TP}_3)+(\\text{FP}_1 + \\text{FP}_2 +\\text{FP}_3)}$$\n",
        "\n",
        "$$\\text{Macro Precision} = \\frac{P_1 + P_2 + P_3}{3} = P_1 * \\frac{1}{3} + P_2 * \\frac{1}{3} + P_3 * \\frac{1}{3}$$\n",
        "\n",
        "$$\\text{Weighted Precision} = P_1 * \\frac{S_1}{N} + P_2 * \\frac{S_2}{N} + P_3 * \\frac{S_3}{N}$$\n",
        "\n",
        "Where:\n",
        "- $S$ is the support for the class (i.e. number of observations with that labels)\n",
        "- $N$ is the total number of observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taAbtjDPawqg"
      },
      "source": [
        "## 3. Classification Experiments with Scikit-Learn\n",
        "\n",
        "- Loading Data\n",
        "- Baselines\n",
        "- Training Classifier\n",
        "- Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLiu9JgLawqh"
      },
      "source": [
        "### 3.1. Loading and Inspecting a Dataset\n",
        "\n",
        "`scikit-learn` comes with several toy datasets.\n",
        "Let's use one of those (iris) to perform a simple classification experiment.\n",
        "\n",
        "The iris dataset is a classic and very easy multi-class classification dataset.\n",
        "\n",
        "| Property          | Value |\n",
        "|-------------------|-------|\n",
        "| Classes           |   3 |\n",
        "| Samples per class |  50 |\n",
        "| Samples total     | 150 |\n",
        "| Dimensionality    |   4 | \n",
        "| Features          | real, positive | "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndGZGwZgawqi",
        "outputId": "06f1362d-49ab-4217-d47a-d7630ba213d6"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from collections import Counter\n",
        "data = load_iris()\n",
        "\n",
        "print(\"Classes: {}\".format(len(list(data.target_names))))\n",
        "print(\"Samples: {}\".format(len(data.data)))\n",
        "print(\"Dimensionality: {}\".format(len(list(data.feature_names))))\n",
        "print(\"Samples per Class: {}\".format(dict(Counter(list(data.target)))))\n",
        "\n",
        "print(data.data[0])  # prints feature vector\n",
        "\n",
        "print(data.data.shape)  # prints matrix shape for data\n",
        "print(data.target.shape)  # print matrix shape for labels\n",
        "\n",
        "# print(data.DESCR)  # prints full data set description\n",
        "# print(data.data)  # prints features\n",
        "# print(data.target) # prints labels"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes: 3\n",
            "Samples: 150\n",
            "Dimensionality: 4\n",
            "Samples per Class: {0: 50, 1: 50, 2: 50}\n",
            "[5.1 3.5 1.4 0.2]\n",
            "(150, 4)\n",
            "(150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF0b6IJ0awqk"
      },
      "source": [
        "### 3.2. Splitting the Dataset\n",
        "\n",
        "- Random K-Fold Split\n",
        "- Stratified K-Fold Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ws0Yw3Zawqk",
        "outputId": "646bdf50-6b8f-4269-f256-3d23c13a40fe"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "random_split = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "for train_index, test_index in random_split.split(data.data):\n",
        "    \n",
        "    print(\"Samples per Class in Training: {}\".format(dict(Counter(list(data.target[train_index])))))\n",
        "    print(\"Samples per Class in Testing: {}\".format(dict(Counter(list(data.target[test_index])))))\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples per Class in Training: {0: 41, 1: 40, 2: 39}\n",
            "Samples per Class in Testing: {0: 9, 1: 10, 2: 11}\n",
            "Samples per Class in Training: {0: 39, 1: 42, 2: 39}\n",
            "Samples per Class in Testing: {0: 11, 1: 8, 2: 11}\n",
            "Samples per Class in Training: {0: 41, 1: 37, 2: 42}\n",
            "Samples per Class in Testing: {0: 9, 1: 13, 2: 8}\n",
            "Samples per Class in Training: {0: 39, 1: 44, 2: 37}\n",
            "Samples per Class in Testing: {0: 11, 1: 6, 2: 13}\n",
            "Samples per Class in Training: {0: 40, 1: 37, 2: 43}\n",
            "Samples per Class in Testing: {0: 10, 1: 13, 2: 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOlLelgVawql",
        "outputId": "f1fa498f-2442-4952-d07c-c3419dfca17c"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "stratified_split = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
        "    \n",
        "    print(\"Samples per Class in Training: {}\".format(dict(Counter(list(data.target[train_index])))))\n",
        "    print(\"Samples per Class in Testing: {}\".format(dict(Counter(list(data.target[test_index])))))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples per Class in Training: {0: 40, 1: 40, 2: 40}\n",
            "Samples per Class in Testing: {0: 10, 1: 10, 2: 10}\n",
            "Samples per Class in Training: {0: 40, 1: 40, 2: 40}\n",
            "Samples per Class in Testing: {0: 10, 1: 10, 2: 10}\n",
            "Samples per Class in Training: {0: 40, 1: 40, 2: 40}\n",
            "Samples per Class in Testing: {0: 10, 1: 10, 2: 10}\n",
            "Samples per Class in Training: {0: 40, 1: 40, 2: 40}\n",
            "Samples per Class in Testing: {0: 10, 1: 10, 2: 10}\n",
            "Samples per Class in Training: {0: 40, 1: 40, 2: 40}\n",
            "Samples per Class in Testing: {0: 10, 1: 10, 2: 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB7M1njzawqm"
      },
      "source": [
        "### 3.3. Training and Testing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_-7rp45awqm"
      },
      "source": [
        "#### 3.3.1. Classification Process\n",
        "\n",
        "- Select the classification algorithm from [Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html)\n",
        "- Train on training data\n",
        "- Predict labels on testing data\n",
        "- Score prediction comparing predicted and reference labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PE8Ygz8awqn",
        "outputId": "e8ca75ee-240f-40df-9e51-20927ae3b18f"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# choose classification algorithm & initialize it\n",
        "clf = GaussianNB()\n",
        "\n",
        "# for each training/testing fold\n",
        "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
        "    # train (fit) model\n",
        "    clf.fit(data.data[train_index], data.target[train_index])\n",
        "    # predict test labels\n",
        "    clf.predict(data.data[test_index])\n",
        "    # score the model (using average accuracy for now)\n",
        "    accuracy = clf.score(data.data[test_index], data.target[test_index])\n",
        "    print(\"Accuracy: {:.3}\".format(accuracy))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.967\n",
            "Accuracy: 0.9\n",
            "Accuracy: 1.0\n",
            "Accuracy: 0.933\n",
            "Accuracy: 0.967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_walZTCcawqn"
      },
      "source": [
        "#### 3.3.2. Baselines\n",
        "\n",
        "Scikit-learn provides baselines via `DummyClassifier` class that takes `strategy` argument. The following baselines can be obtaing:\n",
        "\n",
        "- random baseline: `uniform`\n",
        "- chance baseline: `stratified`\n",
        "- majority baseline: `most_frequent`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbojSLtmawqo",
        "outputId": "23f9ff62-3d89-4ef2-d51c-f63ef5403786"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "random_clf = DummyClassifier(strategy=\"uniform\")\n",
        "\n",
        "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
        "    random_clf.fit(data.data[train_index], data.target[train_index])\n",
        "    random_clf.predict(data.data[test_index])\n",
        "    accuracy = random_clf.score(data.data[test_index], data.target[test_index])\n",
        "    \n",
        "    print(\"Accuracy: {:.3}\".format(accuracy))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.367\n",
            "Accuracy: 0.333\n",
            "Accuracy: 0.333\n",
            "Accuracy: 0.467\n",
            "Accuracy: 0.267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTjlAp63awqo"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "Try `stratified` and `most_frequent` strategies and observe performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amFGOe2-awqp"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA3hnnUdawqr"
      },
      "source": [
        "#### 3.3.3. Better Classification Report\n",
        "\n",
        "scikit-learn provides functions to report more informative performance values using [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knzzh8-cawqs",
        "outputId": "2c6a77f2-ef7f-4eff-a4d2-7b9d1fb5c4f7"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# choose classification algorithm & initialize it\n",
        "clf = GaussianNB()\n",
        "\n",
        "# for each training/testing fold\n",
        "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
        "    # train (fit) model\n",
        "    clf.fit(data.data[train_index], data.target[train_index])\n",
        "    # predict test labels\n",
        "    hyps = clf.predict(data.data[test_index])\n",
        "    refs = data.target[test_index]\n",
        "    \n",
        "    report = classification_report(refs, hyps, target_names=data.target_names)\n",
        "    \n",
        "    print(report)\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00        10\n",
            "   virginica       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.90      0.95        10\n",
            "   virginica       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.90      0.90      0.90        10\n",
            "   virginica       0.90      0.90      0.90        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.93      0.93      0.93        30\n",
            "weighted avg       0.93      0.93      0.93        30\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.90      0.95        10\n",
            "   virginica       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.77      1.00      0.87        10\n",
            "   virginica       1.00      0.70      0.82        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.92      0.90      0.90        30\n",
            "weighted avg       0.92      0.90      0.90        30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ur2DXoawqu"
      },
      "source": [
        "#### 3.3.4. Cross-Validation Evaluation\n",
        "\n",
        "The cross-validation procedure and function of scikit-learn are described in [the documentation](https://scikit-learn.org/stable/modules/cross_validation.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFL_6mYSawqv",
        "outputId": "7632cf0a-c7c7-455b-88c4-99fb265a66d0"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# choose classification algorithm & initialize it\n",
        "clf = GaussianNB()\n",
        "# get scores\n",
        "scores = cross_val_score(clf, data.data, data.target, cv=5)\n",
        "\n",
        "print(scores)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5QC8On6awqw"
      },
      "source": [
        "Cross-Validation using custom split and scoring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDalLHrIawqx",
        "outputId": "2fca782b-c4ad-4204-cd0f-eff20c832974"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# choose classification algorithm & initialize it\n",
        "clf = GaussianNB()\n",
        "# scoring providing our custom split & scoring using \n",
        "scores = cross_validate(clf, data.data, data.target, cv=stratified_split, scoring=['f1_macro'])\n",
        "\n",
        "print(sum(scores['test_f1_macro'])/len(scores['test_f1_macro']))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9598319029897977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey2k27Bgawqx"
      },
      "source": [
        "#### Exercise\n",
        "- Read documentation\n",
        "- Try different evaluation scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNNsvOv0awqy"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upFumjcxawqy"
      },
      "source": [
        "### 3.4. Vectorizing Text\n",
        "\n",
        "> The raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
        "\n",
        "Consequently, the additional step that text classification requires is vectorization that converts text into a vector of numerical values. `scikit-learn` provides several vectorization methods in `sklearn.feature_extraction` [module](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Most commonly used ones are:\n",
        "\n",
        "- Count Vectorization\n",
        "- TF-IDF Vectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrSZxRcawqz"
      },
      "source": [
        "#### 3.4.1. Bag-of-Words Representation\n",
        "\n",
        "[Count Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) implements the following vectorization procedure. \n",
        "\n",
        "- *tokenizing* strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
        "\n",
        "- *counting* the occurrences of tokens in each document.\n",
        "\n",
        "- *normalizing* and *weighting* with diminishing importance tokens that occur in the majority of samples / documents.\n",
        "\n",
        "Each token is considered to be a __feature__ and the vector of all the token frequencies for a given document is considered a multivariate __sample__. Consequently, a corpus of documents is represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
        "\n",
        "> If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTzCy2sZawq0"
      },
      "source": [
        "The [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) implements both tokenization and occurrence counting in a single class, and it is possible to provide many parameters. \n",
        "\n",
        "It can take an external preprocessor or perform the following preprocessing steps (read documentation for details):\n",
        "\n",
        "- __strip_accents__: remove accents and perform other character normalization during the preprocessing step.\n",
        "- __lowercase__: convert all characters to lowercase before tokenizing.\n",
        "- __stop_words__: apply a built-in stop word list for English is used. \n",
        "- __token_pattern__: regular expression denoting what constitutes a *token* for tokenization\n",
        "- __ngram_range__: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. (We will see ngrams the next lab)\n",
        "- __max_df__: maximum frequency cut-off: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). \n",
        "- __min_df__: minimum frequency cut-off: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
        "- __vocabulary__: externally provided vocabulary\n",
        "- __binary__: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvUW-UOkawq1"
      },
      "source": [
        "#### 3.4.2. [TF-IDF Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)  \n",
        "TF-IDF Vectorization = Count Vectorization + TF-IDF Transformation\n",
        "\n",
        "> Transforms a count matrix to a normalized tf or tf-idf representation\n",
        "\n",
        "> __Tf__ means term-frequency while __tf-idf__ means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
        "\n",
        "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
        "\n",
        "(Please refer to the documentation for the transformation formulas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuNbCpbMawq1"
      },
      "source": [
        "#### 3.4.3. Vectorization Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEJvQL9Qawq1",
        "outputId": "25f46385-4b31-48e0-defb-df115cebbecf"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'who plays luke on star wars new hope',\n",
        "    'show credits for the godfather',\n",
        "    'who was the main actor in the exorcist',\n",
        "    'find the female actress from the movie she \\'s the man',\n",
        "    'who played dory on finding nemo'\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# use fit_transform to 'learn' the features and vectorize the data\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectors.toarray())  # print numpy vectors"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1]\n",
            " [0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            " [1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 2 0 1 1]\n",
            " [0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 3 0 0 0]\n",
            " [0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJojGT5vawq2",
        "outputId": "f8900393-55db-4cc7-981d-60d9a61e29b1"
      },
      "source": [
        "test_corpus = [\n",
        "    'who was the female lead in resident evil',\n",
        "    'who played guido in life is beautiful'\n",
        "]\n",
        "\n",
        "# 'trained' vectorizer can be later used to transform the test set \n",
        "test_vectors = vectorizer.transform(test_corpus)\n",
        "print(test_vectors.toarray())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWWUyu9gawq2"
      },
      "source": [
        "#### Text Classification Exercise\n",
        "\n",
        "Using Newsgroup dataset from `scikit-learn` train and evaluate Multinomial Naive Bayes model\n",
        "Experiment with different vectorization methods and paramenters:\n",
        "    - `binary` of Count Vecrorization\n",
        "    - TF-IDF Transformation\n",
        "    - min and max cut-offs\n",
        "    - using stop-words\n",
        "    - lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZU95fCAawq3",
        "outputId": "f8f65a63-d802-478e-e6fc-735bc31b0581"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "trn_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "tst_vectors = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "classifier.fit(trn_vectors, newsgroups_train.target)\n",
        "predictions = classifier.predict(tst_vectors)\n",
        "\n",
        "print(classification_report(newsgroups_test.target, predictions, target_names=newsgroups_train.target_names))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.79      0.77      0.78       319\n",
            "           comp.graphics       0.67      0.74      0.70       389\n",
            " comp.os.ms-windows.misc       0.20      0.00      0.01       394\n",
            "comp.sys.ibm.pc.hardware       0.56      0.77      0.65       392\n",
            "   comp.sys.mac.hardware       0.84      0.75      0.79       385\n",
            "          comp.windows.x       0.65      0.84      0.73       395\n",
            "            misc.forsale       0.93      0.65      0.77       390\n",
            "               rec.autos       0.87      0.91      0.89       396\n",
            "         rec.motorcycles       0.96      0.92      0.94       398\n",
            "      rec.sport.baseball       0.96      0.87      0.91       397\n",
            "        rec.sport.hockey       0.93      0.96      0.95       399\n",
            "               sci.crypt       0.67      0.95      0.78       396\n",
            "         sci.electronics       0.79      0.66      0.72       393\n",
            "                 sci.med       0.87      0.82      0.85       396\n",
            "               sci.space       0.83      0.89      0.86       394\n",
            "  soc.religion.christian       0.70      0.96      0.81       398\n",
            "      talk.politics.guns       0.69      0.91      0.79       364\n",
            "   talk.politics.mideast       0.85      0.94      0.89       376\n",
            "      talk.politics.misc       0.58      0.63      0.60       310\n",
            "      talk.religion.misc       0.89      0.33      0.49       251\n",
            "\n",
            "                accuracy                           0.77      7532\n",
            "               macro avg       0.76      0.76      0.75      7532\n",
            "            weighted avg       0.76      0.77      0.75      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "6tTPSQaysn0V",
        "outputId": "52755acf-db3a-4fa0-8e83-7377feff6148"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame({'data': newsgroups_train.data, 'target': newsgroups_train.target})\n",
        "train_df.head()\n",
        "test_df = pd.DataFrame({'data': newsgroups_test.data, 'target': newsgroups_test.target})\n",
        "test_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: Rick Miller &lt;rick@ee.uwm.edu&gt;\\nSubject: ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: R...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: bakken@cs.arizona.edu (Dave Bakken)\\nSub...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  target\n",
              "0  From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. ...       7\n",
              "1  From: Rick Miller <rick@ee.uwm.edu>\\nSubject: ...       5\n",
              "2  From: mathew <mathew@mantis.co.uk>\\nSubject: R...       0\n",
              "3  From: bakken@cs.arizona.edu (Dave Bakken)\\nSub...      17\n",
              "4  From: livesey@solntze.wpd.sgi.com (Jon Livesey...      19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "GsNvSZJ9taoF",
        "outputId": "1eb9e56f-d957-4587-9134-8267500a98ae"
      },
      "source": [
        "train_df.data[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMGKno0Aq4x4",
        "outputId": "300bf962-5ea7-45e0-bf19-4da7d69f8a2e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
        "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
        "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "spaCy: 326\n",
            "NLTK: 179\n",
            "sklearn: 318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZFCpLUBSrge6",
        "outputId": "b63aa6ef-534e-4204-f24c-1a56fd18962f"
      },
      "source": [
        "# Text preprocessing steps - remove numbers, captial letters and punctuation\n",
        "import re\n",
        "import string\n",
        "\n",
        "alphanumeric = lambda x: re.sub(r\"\"\"\\w*\\d\\w*\"\"\", ' ', x)\n",
        "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
        "\n",
        "train_df['data'] = train_df.data.map(alphanumeric).map(punc_lower)\n",
        "train_df.head()\n",
        "\n",
        "test_df['data'] = test_df.data.map(alphanumeric).map(punc_lower)\n",
        "test_df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>from    ubvmsd cc buffalo edu  neil b  gandler...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>from  rick miller  rick ee uwm edu \\nsubject  ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>from  mathew  mathew mantis co uk \\nsubject  r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>from  bakken cs arizona edu  dave bakken \\nsub...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>from  livesey solntze wpd sgi com  jon livesey...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  target\n",
              "0  from    ubvmsd cc buffalo edu  neil b  gandler...       7\n",
              "1  from  rick miller  rick ee uwm edu \\nsubject  ...       5\n",
              "2  from  mathew  mathew mantis co uk \\nsubject  r...       0\n",
              "3  from  bakken cs arizona edu  dave bakken \\nsub...      17\n",
              "4  from  livesey solntze wpd sgi com  jon livesey...      19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tw8OADnuejw",
        "outputId": "b17a45ff-d693-44da-ed69-fe5225fc38aa"
      },
      "source": [
        "train_df['data']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        from  lerxst wam umd edu  where s my thing \\ns...\n",
              "1        from  guykuo carson u washington edu  guy kuo ...\n",
              "2        from  twillis ec ecn purdue edu  thomas e will...\n",
              "3        from  jgreen amber  joe green \\nsubject  re  w...\n",
              "4        from  jcm head cfa harvard edu  jonathan mcdow...\n",
              "                               ...                        \n",
              "11309    from  jim zisfein factory com  jim zisfein  \\n...\n",
              "11310    from  ebodin pearl tufts edu\\nsubject  screen ...\n",
              "11311    from  westes netcom com  will estes \\nsubject ...\n",
              "11312    from  steve hcrlgw  steven collins \\nsubject  ...\n",
              "11313    from  gunning cco caltech edu  kevin j  gunnin...\n",
              "Name: data, Length: 11314, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1OswyfKtHSW"
      },
      "source": [
        "train_df['data'] = [word for word in train_df['data'] if not word in stopwords.words()]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrVAjq-Fvwa-"
      },
      "source": [
        "test_df['data'] = [word for word in test_df['data'] if not word in stopwords.words()]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "frZOSepzug3c",
        "outputId": "826ab545-3758-4243-ff86-6ae77fd6c49b"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>from  lerxst wam umd edu  where s my thing \\ns...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>from  guykuo carson u washington edu  guy kuo ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>from  twillis ec ecn purdue edu  thomas e will...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>from  jgreen amber  joe green \\nsubject  re  w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>from  jcm head cfa harvard edu  jonathan mcdow...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  target\n",
              "0  from  lerxst wam umd edu  where s my thing \\ns...       7\n",
              "1  from  guykuo carson u washington edu  guy kuo ...       4\n",
              "2  from  twillis ec ecn purdue edu  thomas e will...       4\n",
              "3  from  jgreen amber  joe green \\nsubject  re  w...       1\n",
              "4  from  jcm head cfa harvard edu  jonathan mcdow...      14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrEfLgONu4hp",
        "outputId": "b71a67de-8906-41f4-9472-be0a7f864d59"
      },
      "source": [
        "# Extracting features from text files\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "X_train_cv = count_vect.fit_transform(train_df.data)  # fit_transform learns the vocab and one-hot encodes\n",
        "X_test_cv = count_vect.transform(test_df.data) # transform uses the same vocab and one-hot encodes\n",
        "\n",
        "print(X_train_cv.shape)\n",
        "print(type(X_train_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 81918)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "TAn8NgvWwFRI",
        "outputId": "1699e8fa-7afc-4fc5-84c2-be1f0834f0ea"
      },
      "source": [
        "X_train_cv_df = pd.DataFrame(X_train_cv.todense())\n",
        "X_train_cv_df.columns = sorted(count_vect.vocabulary_)\n",
        "X_train_cv_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaa</th>\n",
              "      <th>aaaa</th>\n",
              "      <th>aaaaaaaaaaaa</th>\n",
              "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugggggggggggggggg</th>\n",
              "      <th>aaaaagggghhhh</th>\n",
              "      <th>aaaarrgghhhh</th>\n",
              "      <th>aaah</th>\n",
              "      <th>aaahh</th>\n",
              "      <th>aaahhhh</th>\n",
              "      <th>aaai</th>\n",
              "      <th>aab</th>\n",
              "      <th>aacc</th>\n",
              "      <th>aachen</th>\n",
              "      <th>aacvkc</th>\n",
              "      <th>aad</th>\n",
              "      <th>aaf</th>\n",
              "      <th>aah</th>\n",
              "      <th>aai</th>\n",
              "      <th>aalac</th>\n",
              "      <th>aalborg</th>\n",
              "      <th>aaldoubo</th>\n",
              "      <th>aalternate</th>\n",
              "      <th>aam</th>\n",
              "      <th>aamazing</th>\n",
              "      <th>aamir</th>\n",
              "      <th>aammmaaaazzzzzziinnnnggggg</th>\n",
              "      <th>aamrl</th>\n",
              "      <th>aams</th>\n",
              "      <th>aan</th>\n",
              "      <th>aanbieden</th>\n",
              "      <th>aanerud</th>\n",
              "      <th>aangeboden</th>\n",
              "      <th>aangegeven</th>\n",
              "      <th>aangezien</th>\n",
              "      <th>aanp</th>\n",
              "      <th>aantal</th>\n",
              "      <th>aao</th>\n",
              "      <th>aaoepp</th>\n",
              "      <th>aap</th>\n",
              "      <th>...</th>\n",
              "      <th>zyr</th>\n",
              "      <th>zyra</th>\n",
              "      <th>zysec</th>\n",
              "      <th>zysv</th>\n",
              "      <th>zyt</th>\n",
              "      <th>zyu</th>\n",
              "      <th>zyv</th>\n",
              "      <th>zyxel</th>\n",
              "      <th>zz</th>\n",
              "      <th>zzcrm</th>\n",
              "      <th>zzd</th>\n",
              "      <th>zzneu</th>\n",
              "      <th>zznki</th>\n",
              "      <th>zznkj</th>\n",
              "      <th>zznkjz</th>\n",
              "      <th>zznkzz</th>\n",
              "      <th>zznp</th>\n",
              "      <th>zzo</th>\n",
              "      <th>zzrk</th>\n",
              "      <th>zzt</th>\n",
              "      <th>zztop</th>\n",
              "      <th>zzz</th>\n",
              "      <th>zzzoh</th>\n",
              "      <th>zzzz</th>\n",
              "      <th>zzzzzz</th>\n",
              "      <th>zzzzzzt</th>\n",
              "      <th>ªl</th>\n",
              "      <th>³ation</th>\n",
              "      <th>ºnd</th>\n",
              "      <th>çait</th>\n",
              "      <th>çon</th>\n",
              "      <th>ère</th>\n",
              "      <th>ée</th>\n",
              "      <th>égligent</th>\n",
              "      <th>élangea</th>\n",
              "      <th>érale</th>\n",
              "      <th>ête</th>\n",
              "      <th>íålittin</th>\n",
              "      <th>ñaustin</th>\n",
              "      <th>ýé</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81918 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aa  aaa  aaaa  aaaaaaaaaaaa  ...  ête  íålittin  ñaustin  ýé\n",
              "0   0    0     0             0  ...    0         0        0   0\n",
              "1   0    0     0             0  ...    0         0        0   0\n",
              "2   0    0     0             0  ...    0         0        0   0\n",
              "3   0    0     0             0  ...    0         0        0   0\n",
              "4   0    0     0             0  ...    0         0        0   0\n",
              "\n",
              "[5 rows x 81918 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFXOCvtYwIu0",
        "outputId": "1b21ed9e-61ea-4e87-97a3-7fb09760e854"
      },
      "source": [
        "# Creating a document-term matrix using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfV = TfidfVectorizer(stop_words='english') \n",
        "# tfidfV = TfidfVectorizer(ngram_range=(1, 2), binary =True, stop_words='english') \n",
        "\n",
        "X_train_tfidfV = tfidfV.fit_transform(train_df.data) # fit_transform learns the vocab and one-hot encodes \n",
        "X_test_tfidfV = tfidfV.transform(test_df.data) # transform uses the same vocab and one-hot encodes \n",
        "\n",
        "# print the dimensions of the training set (text messages, terms) \n",
        "print(X_train_tfidfV.shape)\n",
        "print(type(X_train_tfidfV))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 81918)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st53-NOgwcIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7da88e30-4fbd-43af-8f66-a8cd94baf4d3"
      },
      "source": [
        "X_train_tfidfV_df = pd.DataFrame(X_train_tfidfV.todense())\n",
        "X_train_tfidfV_df.columns = sorted(tfidfV.vocabulary_)\n",
        "X_train_tfidfV_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aaa</th>\n",
              "      <th>aaaa</th>\n",
              "      <th>aaaaaaaaaaaa</th>\n",
              "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugggggggggggggggg</th>\n",
              "      <th>aaaaagggghhhh</th>\n",
              "      <th>aaaarrgghhhh</th>\n",
              "      <th>aaah</th>\n",
              "      <th>aaahh</th>\n",
              "      <th>aaahhhh</th>\n",
              "      <th>aaai</th>\n",
              "      <th>aab</th>\n",
              "      <th>aacc</th>\n",
              "      <th>aachen</th>\n",
              "      <th>aacvkc</th>\n",
              "      <th>aad</th>\n",
              "      <th>aaf</th>\n",
              "      <th>aah</th>\n",
              "      <th>aai</th>\n",
              "      <th>aalac</th>\n",
              "      <th>aalborg</th>\n",
              "      <th>aaldoubo</th>\n",
              "      <th>aalternate</th>\n",
              "      <th>aam</th>\n",
              "      <th>aamazing</th>\n",
              "      <th>aamir</th>\n",
              "      <th>aammmaaaazzzzzziinnnnggggg</th>\n",
              "      <th>aamrl</th>\n",
              "      <th>aams</th>\n",
              "      <th>aan</th>\n",
              "      <th>aanbieden</th>\n",
              "      <th>aanerud</th>\n",
              "      <th>aangeboden</th>\n",
              "      <th>aangegeven</th>\n",
              "      <th>aangezien</th>\n",
              "      <th>aanp</th>\n",
              "      <th>aantal</th>\n",
              "      <th>aao</th>\n",
              "      <th>aaoepp</th>\n",
              "      <th>aap</th>\n",
              "      <th>...</th>\n",
              "      <th>zyr</th>\n",
              "      <th>zyra</th>\n",
              "      <th>zysec</th>\n",
              "      <th>zysv</th>\n",
              "      <th>zyt</th>\n",
              "      <th>zyu</th>\n",
              "      <th>zyv</th>\n",
              "      <th>zyxel</th>\n",
              "      <th>zz</th>\n",
              "      <th>zzcrm</th>\n",
              "      <th>zzd</th>\n",
              "      <th>zzneu</th>\n",
              "      <th>zznki</th>\n",
              "      <th>zznkj</th>\n",
              "      <th>zznkjz</th>\n",
              "      <th>zznkzz</th>\n",
              "      <th>zznp</th>\n",
              "      <th>zzo</th>\n",
              "      <th>zzrk</th>\n",
              "      <th>zzt</th>\n",
              "      <th>zztop</th>\n",
              "      <th>zzz</th>\n",
              "      <th>zzzoh</th>\n",
              "      <th>zzzz</th>\n",
              "      <th>zzzzzz</th>\n",
              "      <th>zzzzzzt</th>\n",
              "      <th>ªl</th>\n",
              "      <th>³ation</th>\n",
              "      <th>ºnd</th>\n",
              "      <th>çait</th>\n",
              "      <th>çon</th>\n",
              "      <th>ère</th>\n",
              "      <th>ée</th>\n",
              "      <th>égligent</th>\n",
              "      <th>élangea</th>\n",
              "      <th>érale</th>\n",
              "      <th>ête</th>\n",
              "      <th>íålittin</th>\n",
              "      <th>ñaustin</th>\n",
              "      <th>ýé</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81918 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    aa  aaa  aaaa  aaaaaaaaaaaa  ...  ête  íålittin  ñaustin   ýé\n",
              "0  0.0  0.0   0.0           0.0  ...  0.0       0.0      0.0  0.0\n",
              "1  0.0  0.0   0.0           0.0  ...  0.0       0.0      0.0  0.0\n",
              "2  0.0  0.0   0.0           0.0  ...  0.0       0.0      0.0  0.0\n",
              "3  0.0  0.0   0.0           0.0  ...  0.0       0.0      0.0  0.0\n",
              "4  0.0  0.0   0.0           0.0  ...  0.0       0.0      0.0  0.0\n",
              "\n",
              "[5 rows x 81918 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-wSi8sO2gCf",
        "outputId": "64b5cbdb-a22a-42c3-aa28-86335a68db3f"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('clf', MultinomialNB())]) \n",
        "\n",
        "text_clf.fit(train_df.data, train_df.target)\n",
        "print(text_clf.score(test_df.data, test_df.target))\n",
        "print(classification_report(test_df.target, text_clf.predict(test_df.data)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7854487519915029\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.82      0.79       319\n",
            "           1       0.64      0.77      0.70       389\n",
            "           2       0.64      0.02      0.03       394\n",
            "           3       0.54      0.78      0.64       392\n",
            "           4       0.80      0.79      0.80       385\n",
            "           5       0.69      0.82      0.75       395\n",
            "           6       0.92      0.69      0.79       390\n",
            "           7       0.86      0.92      0.89       396\n",
            "           8       0.96      0.92      0.94       398\n",
            "           9       0.95      0.90      0.93       397\n",
            "          10       0.94      0.96      0.95       399\n",
            "          11       0.76      0.94      0.84       396\n",
            "          12       0.78      0.70      0.74       393\n",
            "          13       0.88      0.83      0.85       396\n",
            "          14       0.86      0.90      0.88       394\n",
            "          15       0.76      0.95      0.85       398\n",
            "          16       0.69      0.92      0.79       364\n",
            "          17       0.93      0.90      0.92       376\n",
            "          18       0.59      0.60      0.60       310\n",
            "          19       0.84      0.40      0.54       251\n",
            "\n",
            "    accuracy                           0.79      7532\n",
            "   macro avg       0.79      0.78      0.76      7532\n",
            "weighted avg       0.79      0.79      0.77      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx1knkUx3UN7",
        "outputId": "698773cb-27d9-4843-b4a5-46593b0c0b1f"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline([\n",
        "        ('vect', CountVectorizer(stop_words='english')),\n",
        "        ('clf', MultinomialNB())])  # MultinomialNB, LogisticRegression, SGDClassifier, KNeighborsClassifier\n",
        "\n",
        "text_clf.fit(train_df.data, train_df.target)\n",
        "print(text_clf.score(test_df.data, test_df.target))\n",
        "print(classification_report(test_df.target, text_clf.predict(test_df.data)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8003186404673394\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       319\n",
            "           1       0.62      0.80      0.70       389\n",
            "           2       0.85      0.06      0.10       394\n",
            "           3       0.53      0.78      0.63       392\n",
            "           4       0.80      0.82      0.81       385\n",
            "           5       0.72      0.81      0.76       395\n",
            "           6       0.87      0.73      0.80       390\n",
            "           7       0.88      0.92      0.90       396\n",
            "           8       0.94      0.94      0.94       398\n",
            "           9       0.95      0.94      0.95       397\n",
            "          10       0.94      0.97      0.95       399\n",
            "          11       0.83      0.94      0.88       396\n",
            "          12       0.78      0.71      0.74       393\n",
            "          13       0.89      0.85      0.87       396\n",
            "          14       0.86      0.92      0.89       394\n",
            "          15       0.83      0.94      0.88       398\n",
            "          16       0.70      0.92      0.79       364\n",
            "          17       0.96      0.91      0.93       376\n",
            "          18       0.69      0.61      0.65       310\n",
            "          19       0.83      0.47      0.60       251\n",
            "\n",
            "    accuracy                           0.80      7532\n",
            "   macro avg       0.81      0.79      0.78      7532\n",
            "weighted avg       0.81      0.80      0.78      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIswpb9mwgtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42eea5f6-d5aa-4670-e4df-d270846f4fa1"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Performance with No Stemming & Lemmatization\n",
        "text_clf = Pipeline([\n",
        "        ('vect', TfidfVectorizer(stop_words='english', sublinear_tf=True)), # CountVectorizer, TfidfVectorizer\n",
        "        ('clf', MultinomialNB())])  # MultinomialNB, LogisticRegression, SGDClassifier, KNeighborsClassifier\n",
        "\n",
        "text_clf.fit(train_df.data, train_df.target)\n",
        "print(text_clf.score(test_df.data, test_df.target))\n",
        "print(classification_report(test_df.target, text_clf.predict(test_df.data)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8201009028146574\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.69      0.75       319\n",
            "           1       0.78      0.70      0.74       389\n",
            "           2       0.80      0.71      0.75       394\n",
            "           3       0.63      0.82      0.72       392\n",
            "           4       0.85      0.82      0.83       385\n",
            "           5       0.84      0.81      0.83       395\n",
            "           6       0.89      0.78      0.83       390\n",
            "           7       0.90      0.92      0.91       396\n",
            "           8       0.94      0.95      0.95       398\n",
            "           9       0.94      0.94      0.94       397\n",
            "          10       0.90      0.99      0.94       399\n",
            "          11       0.75      0.96      0.84       396\n",
            "          12       0.85      0.66      0.74       393\n",
            "          13       0.93      0.81      0.87       396\n",
            "          14       0.85      0.94      0.89       394\n",
            "          15       0.64      0.96      0.77       398\n",
            "          16       0.66      0.96      0.78       364\n",
            "          17       0.93      0.95      0.94       376\n",
            "          18       0.95      0.51      0.66       310\n",
            "          19       0.93      0.22      0.35       251\n",
            "\n",
            "    accuracy                           0.82      7532\n",
            "   macro avg       0.84      0.80      0.80      7532\n",
            "weighted avg       0.84      0.82      0.81      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auQ-bisewpPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08dbbae5-ef72-4a3a-e851-eace89f1eed8"
      },
      "source": [
        "# Performance with No Stemming & Lemmatization, and ngram_range = (1,2)\n",
        "text_clf = Pipeline([\n",
        "        ('vect', TfidfVectorizer(stop_words='english', ngram_range = (1,2), binary = True, sublinear_tf=True)), # CountVectorizer, TfidfVectorizer\n",
        "        ('clf', MultinomialNB())])  # BernoulliNB, MultinomialNB, LogisticRegression, SGDClassifier, KNeighborsClassifier\n",
        "\n",
        "text_clf.fit(train_df.data, train_df.target)\n",
        "print(text_clf.score(test_df.data, test_df.target))\n",
        "print(classification_report(test_df.target, text_clf.predict(test_df.data)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8146574614976102\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.72      0.77       319\n",
            "           1       0.76      0.69      0.72       389\n",
            "           2       0.77      0.72      0.74       394\n",
            "           3       0.64      0.78      0.70       392\n",
            "           4       0.85      0.76      0.80       385\n",
            "           5       0.84      0.82      0.83       395\n",
            "           6       0.84      0.85      0.84       390\n",
            "           7       0.88      0.88      0.88       396\n",
            "           8       0.92      0.95      0.94       398\n",
            "           9       0.89      0.92      0.90       397\n",
            "          10       0.85      0.98      0.91       399\n",
            "          11       0.77      0.95      0.85       396\n",
            "          12       0.83      0.67      0.74       393\n",
            "          13       0.89      0.77      0.83       396\n",
            "          14       0.82      0.93      0.87       394\n",
            "          15       0.71      0.96      0.81       398\n",
            "          16       0.69      0.94      0.80       364\n",
            "          17       0.91      0.94      0.92       376\n",
            "          18       0.95      0.50      0.66       310\n",
            "          19       0.96      0.31      0.47       251\n",
            "\n",
            "    accuracy                           0.81      7532\n",
            "   macro avg       0.83      0.80      0.80      7532\n",
            "weighted avg       0.83      0.81      0.81      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLP-eCx2wsE1"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
        "\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "    \n",
        "    def __init__(self, stemmer, *args, **kwargs):\n",
        "        super(StemmedTfidfVectorizer, self).__init__(*args, **kwargs)\n",
        "        self.stemmer = stemmer\n",
        "        \n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
        "        return lambda doc: (self.stemmer.stem(word) for word in analyzer(doc.replace('\\n', ' ')))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auE3y7sUx8Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7017b89-6751-4106-ca59-086770ae9b65"
      },
      "source": [
        "# Performance of NB Classifier with Stemming\n",
        "text_clf = Pipeline([\n",
        "        ('vect', StemmedTfidfVectorizer(stemmer=stemmer, stop_words='english', sublinear_tf=True)), # CountVectorizer, TfidfVectorizer\n",
        "        ('clf', MultinomialNB())])  # MultinomialNB, LogisticRegression, SGDClassifier, KNeighborsClassifier\n",
        "\n",
        "text_clf.fit(train_df.data, train_df.target)\n",
        "print(text_clf.score(test_df.data, test_df.target))\n",
        "print(classification_report(test_df.target, text_clf.predict(test_df.data)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8138608603292619\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.67      0.74       319\n",
            "           1       0.80      0.68      0.73       389\n",
            "           2       0.82      0.69      0.75       394\n",
            "           3       0.65      0.80      0.71       392\n",
            "           4       0.85      0.82      0.83       385\n",
            "           5       0.84      0.82      0.83       395\n",
            "           6       0.90      0.75      0.82       390\n",
            "           7       0.89      0.92      0.91       396\n",
            "           8       0.94      0.96      0.95       398\n",
            "           9       0.95      0.94      0.94       397\n",
            "          10       0.91      0.99      0.95       399\n",
            "          11       0.70      0.96      0.81       396\n",
            "          12       0.83      0.66      0.73       393\n",
            "          13       0.92      0.81      0.86       396\n",
            "          14       0.85      0.95      0.90       394\n",
            "          15       0.61      0.97      0.75       398\n",
            "          16       0.65      0.95      0.77       364\n",
            "          17       0.90      0.95      0.92       376\n",
            "          18       0.96      0.48      0.64       310\n",
            "          19       0.94      0.18      0.31       251\n",
            "\n",
            "    accuracy                           0.81      7532\n",
            "   macro avg       0.84      0.80      0.79      7532\n",
            "weighted avg       0.83      0.81      0.81      7532\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO11Fsm_1u6D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}